###  前馈网络

- 前馈
  - 信息流动方向

### 同步和异步机制

### 同步：全局时钟

- **逻辑**：每一层必须等前一层所有神经元计算完成后，再统一进行本层的计算。

- **全局时钟**：类似于计算机 CPU 的时钟周期。在深度学习中，这通常表现为 **Mini-batch** 训练，所有样本计算完平均梯度后统一更新权重。

- **优点**：数学严格性强，梯度更新稳定。
- 横向类比： barrier屏障同步机制

### 异步：时间戳

- **逻辑**：不需要等待。某个神经元或计算节点只要完成计算，就可以立即传给下一层或更新全局参数。

- **时间戳 (Timestamp)**：由于更新不再同步，需要用时间戳来标记参数的“陈旧度”（Staleness），防止旧梯度破坏了新参数。

- **应用场景**：大规模分布式训练（如参数服务器架构 Parameter Server）。

## 权重初始化

### **禁止等值初始化**

- 网络对称性： 同一层的所有神经元在前向传播时接收相同的输入，在反向传播时计算出完全相同的梯度。无论有多少个神经元，它们永远在做重复的更新。神经网络会**退化成一个“单神经元”模型**，失去了学习复杂特征的能力。

### 常见的两类的初始化

- 随机均匀分布
- 随机正态分布

### Xavier初始化

- 保证输入输出流的信息能量一致
- 基于线性激活函数

### He初始化

- 基于ReLU激活
- 由于ReLU使得信息能量流减半，所以初始化采用了信息流放大一倍的功能

### 信息能量

####  信息 能量模型和Xavier初始化

假设某一层神经元的输出为 $y = \sum w_i x_i$。根据概率统计，输出的方差（能量）满足：
$$
Var(y) = Var(\sum_{i=1}^{n} w_ix_i)=n \cdot Var(w) \cdot Var(x)
$$
为了让能量在传递中不衰减（即 $Var(y) = Var(x)$），我们需要让 $Var(w) = \frac{1}{n}$。这就是 **Xavier 初始化**（针对线性激活）的基础。

#### ReLU的能量影响

当加入 ReLU 激活函数 $f(x) = \max(0, x)$ 后，情况发生了变化：

- **负半区截断**：由于 ReLU 强行将一半的输入信号变为 0，在统计上，经过 ReLU 后的信号方差会减小一半。
- **数学表达**：$Var(f(y)) \approx \frac{1}{2} Var(y)$​。

#### He 初始化：放大能量

既然 ReLU 会吞掉一半的能量，那么在权重初始化阶段，我们就提前把能量**放大一倍**。

- **公式对比**：
  - **Xavier**: $Var(W) = \frac{1}{n}$
  - **He (Kaiming)**: $Var(W) = \frac{2}{n}$
- **结论**：通过将分子的 1 变为 2，He 初始化抵消了 ReLU 带来的 50% 能量损失，确保深层网络中的信号既不消失也不爆炸。