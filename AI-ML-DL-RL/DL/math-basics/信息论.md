## 信息量、意外程度和语境

* 语境/概率->意外程度/信息量

* 什么是信息

  * 一种算法
    * 输入：事件，概率密度函数（事件意外程度的表示）
    * 输出：信息量（数值，能够反映事件的可能性
    * 满足的条件
      * 可能性（省略三条，DLFBP:6.4
      * 两个**不相关**信息的总信息量是各自信息量之和

* 数学

  * 自信息。对于一个特定的事件 $x$，它发生的概率为 $P(x)$，那么这个事件所包含的信息量 $I(x)$ 用以下公式表示：

    * $$
      I(x)=log_2(\frac{1}{P(x)})
      $$

    * 概率越小，信息量越大

    * 对数：因为信息可以相加

  * 信息熵 (Entropy)。用于评估某个“信号源”平均有多少信息量，我们会用**信息熵 $H(X)$​** 来表示

    * $$H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)$$​
    * 乘以一个P(x)：可以理解为加权均值
    * 熵最大：所有时间的概率相等

  * 单位： bit

    * **0 bit：** 100% 会发生的事（如：明天太阳升起）。
    * **1 bit：** 二选一且概率相等的事（如：硬币正反）。
    * **约 2 bit：** 四选一且概率相等的事（如：抽中扑克牌里的某个花色）。
    * **约 12 bit：** 在 4000 个常用汉字中猜中一个特定的字（如果没有语境提示）

  

  ## 摩斯密码改进

  * 音高（声音频率）替换长短
  * 自适应编码：哈夫曼编码原理
    * 压缩比
      * 效率高于定长编码

  

## 两种编码与真实文本

### KL散度

* 当你用一个不完美的模型 $Q$ （比如一种概率分布函数）去近似真实情况 $P$​ 时，损失了多少信息，或者说增加了多少多余的代价
* $$D_{KL}(P || Q) = \sum P(x) \log \frac{P(x)}{Q(x)}$$
  * P（x）乘：加权
  * log相减：（自信息相减）代价

### 交叉熵

* 可以用于检验一个模型Q是否适合用来代表一个真实情况P

  * 常见的有
    * Q是预测模型，P是真实数据
    * Q是编码模型，P是某一本文本小说

* $$H(P, Q) = -\sum_{i=1}^{n} P(x_i) \log_2 Q(x_i)$$

  * $$\text{交叉熵} = \text{信息熵} + \text{相对熵 (KL 散度)}$$

  * **信息熵**：真实世界本身固有的不确定性（无法消除的底噪）。

    **相对熵 (KL Divergence)**：由于你的预测模型不准，而额外产生的冗余信息量。

    