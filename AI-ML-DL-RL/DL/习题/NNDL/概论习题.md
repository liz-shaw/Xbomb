# ch1 神经网络概论

## 第一题：使用ReLU激活函数学习异或（XOR）函数

**题目**： 考虑异或（XOR）函数的情况：其中两个点 $\{(0,0), (1,1)\}$ 属于一类，而另外两个点 $\{(1,0), (0,1)\}$ 属于另一类。请展示如何利用 ReLU 激活函数，以类似于 图1.14 中的方式将这两类点分离开来

![image-20251227162004418](../../../../../../../AppData/Roaming/Typora/typora-user-images/image-20251227162004418.png)

**分析**：这是个很有趣的问题，因为这用一种非常直观的方式展示了非线性激活函数的功能：空间折叠。

而我将通过几何的方式来分析。

* 第一个问题： **激活前值ai**代表着什么呢？

  * $$a_i=w_{i1}x_1+w_{i2}x_2+b_i$$，

    *  也许我们可以换一种写法：

      ​	$$0=w_{i1}x_1+w_{i2}x_2+b_i-a_i$$​
      
      上述表达式实际上是一组斜率相同的平行线。对于不同的输入数据(x1,x2)，ai有着不同的值，即意味着截距不同。而激活前值ai实际上代表了输入数据代表的直线在y轴上的截距相关值，而且ai与截距是成反比的

    * 对于线性可分的情况，其实只要找到分界值$a_{id}$​，就可以将数据分为两组
    * 但是对于线性不可分的情况，直线的作用就受到了限制。我们需要引入非线性。那么非线性是如何作用的呢？本题中的ReLU函数，他把所有在直线$0=w_{i1}x_1+w_{i2}x_2+b_i$ 以上的数据都压缩到直线上了，

* 第二个问题： **激活后值hi**代表什么呢？

  * 对于本题中的激活函数ReLU, 任意小于0的值都会被压缩为0
  * 