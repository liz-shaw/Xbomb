# 激活函数

## 激活函数的重要性

- 防止线性深度网络退化成单层

### 通用近似定理

- [通用近似定理（Universal Approximation Theorem）](https://www.geeksforgeeks.org/deep-learning/universal-approximation-theorem-for-neural-networks/)
  - 只要给神经网络足够的神经元（神经元数量可以趋于无穷大），它就能以任意精度拟合任何一个连续函数
- 成立前提
  - **单隐藏层**：最经典版本的定理证明了，你不需要深层网络，哪怕只有**一个**隐藏层，只要足够宽，也行。
  - **非线性激活函数**：如果没有非线性（比如只用线性函数），神经网络无论加多少层，本质上还是个线性方程（直线），永远拟合不了复杂的曲线。
  - **连续性**：目标函数通常需要是在闭区间上的连续函数。
- 局限性
  - 网络规模和效率
  - 过拟合
  - 泛化能力
  - 训练成本和困难

- Q： 使用神经网络能比加入了多项式拟合的模型表现效果更好吗？

  - | **特性**     | **多项式拟合**                   | **神经网络**                                     |
    | ------------ | -------------------------------- | ------------------------------------------------ |
    | **局部性**   | 改变一个系数会影响全局曲线       | 神经元通常只对输入的特定区域敏感（局部响应更好） |
    | **特征工程** | 需要人手动决定用几阶、哪些项交叉 | 自动学习特征表示                                 |
    | **稳定性**   | 高阶时极易过拟合，数值计算不稳定 | 通过正则化（Dropout, BN）和深度结构保持稳定      |
    | **非线性**   | 仅限于幂函数形式的非线性         | 可以模拟任何形式的复杂非线性                     |

  - 什么时候多项式拟合更好
    - 数据量极小
    - 物理背景和建模明确
    - 计算资源有限

## 用于输出端的激活

### 线性激活

### softmax激活（分类

- 指数放大再归一化
- 特点
  - 缺点
    - 过于自信
  - 优点
    - 线性归一化下的均匀分布，softmax也均匀
    - 线性归一化下的不均匀分布，softmax会放大权重占比大的。赢家通吃
- 变体
  - 温度softmax: 指数系数
    - 就是把指数级放缩
    - **$T < 1$**: 概率分布更陡峭（更自信，选择概率最大的词）。
    - **$T > 1$**: 概率分布更平滑（更随机，增加生成多样性）。

## 非平滑激活函数

- 阶跃
  - 阶跃函数
  - 符号函数
- maxout
- 分段线性
  - 比如relu

## ReLU变体

- RELU: RECTIFIER LINEAR UNIT:整流线性单元
  - 源于电路中的整流器，过滤负电压
- relu的优缺点
  - 优点
    - 梯度好计算
    - 在大于0的地方，梯度不会消失
  - 缺点
    - 在小于0的地方，会造成神经元死亡
- leaky relu
  - 在负数部分，保留较小的梯度：0.1
  - parameter relu: 保留梯度比例是个参数
- shifted relu
  - 将relu往左下角移动
  - 激活函数和bias偏置的区别
    - bias本身可以体现对于激活函数的平移， bias可以理解为一个没有固定位置的激活函数形状的激活函数的一个自由度。在输入端的bias体现的是左右的平移，输出端的bias 体现的是上下的平移
- noisy relu
  - 不常用
  - 在输入后，先添加噪声，再进行激活函数
- 指数relu(ELU
  - relu移位（shifted ReLU）平滑
- SiLU(sigmoid linear unit)/swish
  - 平滑
  - swish(x)=x(sigmoid(bx))
- softplus: relu平滑

## 平滑激活函数

- 特点：
  - 优点
    - 可求导
    - 梯度较稳定
  - 缺点
    - 在两端出现梯度消失的情况
- 常见的平滑激活
  - ELU
  - sigmoid
    - $Sigmoid(x) = \frac{1}{1 + e^{-x}}$
    - (0,1压缩
  - tanh
    - $Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ 
    - （-1，1压缩
    - 奇函数
  - sigmoid和relu的组合：swish/Silu
    - swish(x)=x(sigmoid(bx))

## relu激活下的神经元死亡

- 某个神经元，经过它的每个样本的wx+b值都是0的时候，会导致权重一致不更新，从而使得神经元死亡
- 常见的触发场景
  - 学习率太大
  - 权重初始化方案糟糕
    - 详情请看前馈网络章节，里面有提及权重初始化注意事项，Xavier,和He初始化
  - 数据没有归一化
  - 没有bathnorm
  - 早期训练差